### 梯度
关于梯度的学习参考作者的这篇文章：
https://blog.csdn.net/weixin_57128596/article/details/138975635

#### 1.简单易懂
- 梯度消失、爆炸的定义
> 损失对参数更新不动了/疯狂更新
- 公式体现
> 损失对参数进行链式求导
- 常见神经网络中为什么会出现梯度消失？
> 求导后的激活函数值很小，然后时间步太长累乘后会非常小

#### 2.一个例子
首先我们需要知道 RNN 不仅依赖于当前步的输入，还依赖于前一步输出隐藏状态。

**1.一个简单RNN单元的隐藏状态更新公式为：**

\[ h_t = \sigma(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \]

其中：
- \( h_t \)：当前时间步 \( t \) 的隐藏状态。
- \( h_{t-1} \)：前一时间步的隐藏状态。
- \( x_t \)：当前时间步的输入。
- \( W_{hh}, W_{xh} \)：隐藏状态到隐藏状态和输入到隐藏状态的权重矩阵。
- \( b_h \)：偏置项。
- \( \sigma \)：激活函数（如sigmoid或tanh）。

输出通常通过隐藏状态进一步计算：

\[ y_t = W_{hy} h_t + b_y \]

损失函数 \( L \) 通常是对所有时间步的损失求和：

\[ L = \sum_{t=1}^T L_t \]

其中 \( L_t \) 是时间步 \( t \) 的损失，例如均方误差或交叉熵。


**2.目的：计算损失 \( L \) 相对于权重（如 \( W_{hh} \)）的梯度：（当前步的链式求导）**

\[ \frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^T \frac{\partial L_t}{\partial W_{hh}} \]

对于每一时间步 \( t \)，通过链式法则，梯度为：

\[ \frac{\partial L_t}{\partial W_{hh}} = \frac{\partial L_t}{\partial h_t} \cdot \frac{\partial h_t}{\partial W_{hh}} \]

但由于 \( h_t \) 依赖于 \( h_{t-1} \)，而 \( h_{t-1} \) 依赖于 \( h_{t-2} \)，以此类推，梯度计算需要考虑整个时间序列的依赖关系。因此，完整的梯度计算需要展开所有时间步。

**3.计算输出当前隐藏状态ht下对某参数更新的损失还需要考虑之前的隐藏状态ht-1：**
隐藏状态 \( h_t \) 的梯度为：

\[ \frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L_{t+1}}{\partial h_t} + \cdots + \frac{\partial L_T}{\partial h_t} \]

其中，\(\frac{\partial L_t}{\partial h_t}\) 是当前时间步的直接贡献，而后续时间步的贡献通过递归关系传递：（关键！！）

\[ \frac{\partial L_{t+k}}{\partial h_t} = \frac{\partial L_{t+k}}{\partial h_{t+k}} \cdot \frac{\partial h_{t+k}}{\partial h_{t+k-1}} \cdots \frac{\partial h_{t+1}}{\partial h_t} \]

我们发现到最后是隐藏状态对前面隐藏状态的求导！！我们把公式带进去，导数为（复合函数求导）：

\[ h_t = \sigma(z_t), \quad z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h \]

\[ \frac{\partial h_t}{\partial h_{t-1}} = \sigma'(z_t) \cdot W_{hh} \]

因此，跨多个时间步的梯度为：

\[ \frac{\partial h_{t+k}}{\partial h_t} = \prod_{i=t+1}^{t+k} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=t+1}^{t+k} \sigma'(z_i) \cdot W_{hh} \]

**4.梯度消失的数学原因：\(\sigma'(z_i)\) 是关键**

梯度消失的关键在于 \(\sigma'(z_i) \cdot W_{hh}\) 的值。如果这个值的模长期小于1，梯度会随着时间步的增加指数级衰减。（也就是激活函数的影响！！）

1. **Sigmoid激活函数**：
   - Sigmoid函数的导数为：\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\)，最大值在 \( x=0 \) 时为 0.25。
   - 如果 \( W_{hh} \) 的模接近1，梯度在每个时间步会被压缩至原来的 0.25 或更小。

2. **Tanh激活函数**：
   - Tanh的导数为：\(\tanh'(x) = 1 - \tanh^2(x)\)，最大值为1。
   - 比sigmoid略好，但如果 \( W_{hh} \) 的模小于1，梯度仍会逐渐衰减（但很正常，就是要衰减才对，不要太衰就行）。

#### 具体例子
假设我们有一个长度为 \( T=100 \) 的序列，使用sigmoid激活函数，\(\sigma'(z_t) \approx 0.25\)，并且 \( W_{hh} \) 的模为1。跨 \( k \) 个时间步的梯度为：

\[ \frac{\partial h_{t+k}}{\partial h_t} \approx (0.25 \cdot 1)^k = 0.25^k \]

当 \( k=100 \) 时：

\[ 0.25^{100} \approx 10^{-60} \]

这个值极小，意味着早期时间步的梯度对最终损失几乎没有影响，导致梯度消失。


### 解决梯度消失的方法

1. **使用ReLU激活函数**：
   - ReLU的导数为1（当输入大于0时），避免了梯度压缩。
   - 但ReLU可能导致梯度爆炸或“死亡神经元”问题。

2. **LSTM/GRU**：
   - LSTM通过遗忘门、输入门和输出门控制信息流，允许梯度以接近恒定的方式传递。
   - GRU是LSTM的简化版，同样有效。

3. **权重初始化**：
   - 使用适当的权重初始化（如Xavier初始化）使 \( W_{hh} \) 的模接近1，减缓梯度衰减。

4. **梯度裁剪**：
   - 限制梯度的最大值，防止梯度爆炸，同时缓解部分梯度消失问题。

5. **残差连接**：
   - 通过添加“跳跃连接”，允许梯度直接传递到早期层，类似ResNet的结构。

---

### 总结

RNN中梯度消失的根本原因是反向传播中跨时间步的梯度相乘，导致梯度指数级衰减。具体来说，激活函数（如sigmoid）的导数小于1以及权重矩阵 \( W_{hh} \) 的模小于1会使梯度快速趋于0。通过数学推导，我们看到梯度计算涉及链式法则的多次乘积，长时间步的贡献几乎为零。通过使用ReLU、LSTM/GRU、权重初始化或残差连接等方法，可以有效缓解这一问题。
